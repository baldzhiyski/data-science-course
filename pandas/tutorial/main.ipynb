{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc98fcfb88c733a9",
   "metadata": {},
   "source": [
    "### Intro to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns=['A','B','C'], index=['x','y','z'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "374fbfbae50103ac",
   "metadata": {},
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "caee5441f1d45e8b",
   "metadata": {},
   "source": [
    "### Loading in Dataframes from Files\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5a9bb54644e188a",
   "metadata": {},
   "source": [
    "coffee = pd.read_csv(\"./warmup-data/coffee.csv\")\n",
    "\n",
    "results = pd.read_parquet(\"./data/results.parquet\")\n",
    "\n",
    "olympics = pd.read_excel(\"./data/olympics-data.xlsx\")\n",
    "\n",
    "bios = pd.read_csv(\"./data/bios.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80bb40bac9167e5e",
   "metadata": {},
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "id": "6bcd02c5f5f30e00",
   "metadata": {},
   "source": [
    "coffee.sample(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e47f995f3ed61991",
   "metadata": {},
   "source": [
    "coffee.loc[0:6,[\"Day\",\"Units Sold\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2434382d65448ee8",
   "metadata": {},
   "source": [
    "coffee.iloc[0:6,[0,2]] # Only using index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d0c9359fe8c682f",
   "metadata": {},
   "source": [
    "coffee.loc[1:4,\"Unit Solds\"] = 10\n",
    "\n",
    "coffee.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3f36e2ba3db2bec",
   "metadata": {},
   "source": [
    "coffee.at[0,\"Unit Solds\"]=100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f243cf1962e05a57",
   "metadata": {},
   "source": [
    "coffee.sort_values(\"Unit Solds\",ascending=True)\n",
    "\n",
    "coffee.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e606dafbc76760f",
   "metadata": {},
   "source": [
    "for idx , row in coffee.iterrows():\n",
    "    print(idx)\n",
    "    print(row)\n",
    "    print(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9734c3d40ebc993",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c10df8eb08b4f4aa",
   "metadata": {},
   "source": [
    "bios.loc[bios['height_cm']>215,[\"name\",\"height_cm\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c500573d91f1a304",
   "metadata": {},
   "source": [
    "bios[(bios[\"height_cm\"]>215)][[\"name\",\"height_cm\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84f56e7cfe78a865",
   "metadata": {},
   "source": [
    "bios[(bios[\"height_cm\"]>215) & (bios[\"born_country\"]==\"USA\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdcd86fd7044f0ae",
   "metadata": {},
   "source": [
    "bios[bios[\"name\"].str.contains(\"Hristo\",case=False)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e244836e5783b72",
   "metadata": {},
   "source": [
    "# Top 5 tallest overall\n",
    "bios.nlargest(5, 'height_cm')[['name', 'height_cm', 'born_country']]  # top 5 tallest rows\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "878f45cb8dd1f17c",
   "metadata": {},
   "source": [
    "# Tallest person in each country (per-group max)\n",
    "bios[bios['height_cm'].eq(bios.groupby('born_country')['height_cm'].transform('max'))][['name', 'born_country', 'height_cm']]  # tallest per country"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7cb60807b7aea83",
   "metadata": {},
   "source": [
    "# Filter by a set of countries\n",
    "bios[bios['born_country'].isin(['USA', 'Germany', 'Bulgaria'])][['name', 'born_country', 'height_cm']]  # only selected countries\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85445ce4360da37f",
   "metadata": {},
   "source": [
    "# Exclude certain countries\n",
    "bios[~bios['born_country'].isin({'USA', 'Canada'})][['name', 'born_country', 'height_cm']].head(10)  # not USA/Canada\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be59a1be07471de1",
   "metadata": {},
   "source": [
    "# Names that start with A or H (case-insensitive)\n",
    "bios[bios['name'].str.contains(r'^(A|H)', case=False, na=False)][['name']].head(10)  # anchors at start\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1840293b392d430",
   "metadata": {},
   "source": [
    "# Duplicate names (possible homonyms)\n",
    "bios[bios['name'].duplicated(keep=False)].sort_values('name')[['name', 'born_country', 'height_cm']]  # all duplicates\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1e6396cd6f8057f",
   "metadata": {},
   "source": [
    "# Data-quality check: names containing digits\n",
    "bios[bios['name'].str.contains(r'\\d', na=False)][['name']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43acfe732545be80",
   "metadata": {},
   "source": [
    "# Shortest person in each country (per-group min via sort+drop_duplicates)\n",
    "(bios.sort_values(['born_country', 'height_cm'])\n",
    "     .drop_duplicates('born_country'))[['name', 'born_country', 'height_cm']]  # shortest per country\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6028cc3bccc3060e",
   "metadata": {},
   "source": [
    "bios.query('born_country== \"USA\" and height_cm > 200')[['name', 'born_country', 'height_cm']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92145bf3319c6111",
   "metadata": {},
   "source": [
    "### Adding / Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "b49778ce6cf77b91",
   "metadata": {},
   "source": [
    "coffee['price'] = 5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "coffee['new_price'] = np.where(coffee['Coffee Type'] == 'Espresso',3.99,4.99)\n",
    "\n",
    "coffee.drop(columns=['price'], inplace=True)\n",
    "\n",
    "coffee.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7464e3535b489ec4",
   "metadata": {},
   "source": [
    "## Pointing to the same memory space\n",
    "coffe_new = coffee\n",
    "\n",
    "## New one , not modifying the original\n",
    "coffe_new = coffee.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c93d28b6ba81e777",
   "metadata": {},
   "source": [
    "coffee.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80ea64830ad158f8",
   "metadata": {},
   "source": [
    "coffee['revenue'] = coffee['Units Sold'] * coffee['new_price']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c60fbe889a0680a0",
   "metadata": {},
   "source": [
    "coffee.rename(columns={'new_price':'price'}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71cd8a978346e1a9",
   "metadata": {},
   "source": [
    "bios.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32913c49e2667d2b",
   "metadata": {},
   "source": [
    "bios['first_name'] = bios['name'].str.split(' ').str[0]\n",
    "\n",
    "\n",
    "bios['born_data_time']=pd.to_datetime(bios['born_date'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "549a2bcbd5621e05",
   "metadata": {},
   "source": [
    "bios['height_category'] = bios['height_cm'].apply(lambda x : 'Short' if x < 165 else ('Average' if x<185 else 'Tall'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0d03b5e24719534",
   "metadata": {},
   "source": [
    "def categorize_athlete(row):\n",
    "    if row['height_cm'] < 175 and row['weight_kg'] < 70:\n",
    "        return 'Lightweight'\n",
    "    elif row['height_cm'] < 185 and row['weight_kg'] <= 80:\n",
    "        return 'Middleweight'\n",
    "    else:\n",
    "        return 'Heavyweight'\n",
    "\n",
    "bios['category'] = bios.apply(categorize_athlete, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49b5a8bd2fa40466",
   "metadata": {},
   "source": [
    "### Merging & Concatenating Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "1723e8b112adac9c",
   "metadata": {},
   "source": [
    "nocs = pd.read_csv('./data/noc_regions.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad0de113778efb69",
   "metadata": {},
   "source": [
    "nocs.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db270c5ec8825dc2",
   "metadata": {},
   "source": [
    "bios = pd.merge(bios, nocs, left_on='born_country', right_on='NOC', how='left')\n",
    "\n",
    "bios.rename(columns={'region':'born_country_full'},inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63c4dea8a56adbb4",
   "metadata": {},
   "source": [
    "usa = bios[bios.born_country=='USA'].copy()\n",
    "gbr = bios[bios.born_country=='GBR'].copy()\n",
    "\n",
    "new_df = pd.concat([usa,gbr])\n",
    "\n",
    "new_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "336b35b9db179224",
   "metadata": {},
   "source": [
    "### Handling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "id": "55217cca72b514e3",
   "metadata": {},
   "source": [
    "coffee.loc[[0,1],'Units Sold'] = np.nan"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dc8303953e272d6",
   "metadata": {},
   "source": [
    "display(coffee)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7abdba71e7cde060",
   "metadata": {},
   "source": [
    "coffee= coffee.fillna(coffee['Units Sold'].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0a530d4b27b1852",
   "metadata": {},
   "source": [
    "coffee.dropna(subset=['Units Sold'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18b112617aa2e9d8",
   "metadata": {},
   "source": [
    "coffee[coffee['Units Sold'].notna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3fee72bdac9a449",
   "metadata": {},
   "source": [
    "### Aggregating Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "b18c53184e06358f",
   "metadata": {},
   "source": [
    "bios.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a84dace1a8bc1c1",
   "metadata": {},
   "source": [
    "bios['born_city'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4205fd93cce0282b",
   "metadata": {},
   "source": [
    "bios[bios['born_country']=='USA']['born_region'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e423f0a2a992ad6",
   "metadata": {},
   "source": [
    "coffee.groupby(['Coffee Type'])['Units Sold'].sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe27a754ef54eb5",
   "metadata": {},
   "source": [
    "coffee.groupby(['Coffee Type']).agg({'Units Sold':'sum','price':'mean'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43b95d651059f5c7",
   "metadata": {},
   "source": [
    "pivot = coffee.pivot(columns='Coffee Type', index='Day' , values = 'revenue')\n",
    "\n",
    "pivot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0aa3056c1790319",
   "metadata": {},
   "source": [
    "bios.groupby(bios['born_date'])['name'].count().reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f737140ab3456a74",
   "metadata": {},
   "source": [
    "# Total, average, min, max height per country\n",
    "bios.groupby('born_country').agg({\n",
    "    'height_cm': ['count', 'mean', 'min', 'max', 'std']\n",
    "})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd7ee18be8040ce",
   "metadata": {},
   "source": [
    "bios.groupby('born_country').agg(\n",
    "    num_people=('name', 'count'),\n",
    "    avg_height=('height_cm', 'mean'),\n",
    "    tallest=('height_cm', 'max'),\n",
    "    shortest=('height_cm', 'min')\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38f7804180110258",
   "metadata": {},
   "source": [
    "# Top 10 countries with highest average height\n",
    "bios.groupby('born_country')['height_cm'].mean().sort_values(ascending=False).head(10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22bbe4185312a653",
   "metadata": {},
   "source": [
    "# Ensure born_country is a column\n",
    "if 'born_country' not in bios.columns:\n",
    "    bios = bios.reset_index()\n",
    "\n",
    "# Make sure height_cm is numeric\n",
    "bios['height_cm'] = pd.to_numeric(bios['height_cm'], errors='coerce')\n",
    "\n",
    "idx = bios.groupby('born_country')['height_cm'].idxmax()\n",
    "idx = idx.dropna().astype(int)   # remove groups where max is NaN\n",
    "\n",
    "out = bios.loc[idx, ['born_country', 'name', 'height_cm']].sort_values('born_country')\n",
    "print(out)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "347232f33bb32dff",
   "metadata": {},
   "source": [
    "### Advanced Funcs"
   ]
  },
  {
   "cell_type": "code",
   "id": "85d68d517a7589ed",
   "metadata": {},
   "source": [
    "coffee['yesterday_revenue'] = coffee['revenue'].shift(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b7fe3fe8321524c",
   "metadata": {},
   "source": [
    "coffee.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2874cfc4393984fb",
   "metadata": {},
   "source": [
    "bios.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "459bb57d7d60347b",
   "metadata": {},
   "source": [
    "bios['height_rank'] = bios['height_cm'].rank()\n",
    "\n",
    "bios.sort_values(['height_cm'],ascending=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1e41900e520fee6",
   "metadata": {},
   "source": [
    "latte = coffee[coffee['Coffee Type']=='Latte'].copy()\n",
    "\n",
    "latte['3day']=latte['Units Sold'].rolling(3).sum()\n",
    "latte"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25f7cd4d67e9f26e",
   "metadata": {},
   "source": [
    "### Other functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "c351eec132ecb29d",
   "metadata": {},
   "source": [
    "results.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e13682a",
   "metadata": {},
   "source": [
    "# Pandas – Advanced Cheatsheet Add‑Ons (New Cells)\n",
    "\n",
    "Below are **new** examples and notes appended to your notebook (your existing content was not modified)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518cabb",
   "metadata": {},
   "source": [
    "# 1) Powerful Indexing & Selection\n",
    "\n",
    "Key patterns beyond basics:\n",
    "\n",
    "- Use `.loc[row_selector, col_selector]` for **label** selection (inclusive slicing), `.iloc` for **position**.\n",
    "- `.at` / `.iat` are fast for single-scalar get/set.\n",
    "- Combine boolean masks, `.isin`, and `.between` for expressive filters.\n",
    "- Set an index for fast lookups: `df = df.set_index('key', drop=False)` keeps column while indexing.\n",
    "- MultiIndex lets you slice blocks of data elegantly."
   ]
  },
  {
   "cell_type": "code",
   "id": "9cdc101d",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"city\": [\"Berlin\",\"Berlin\",\"Munich\",\"Munich\",\"Hamburg\",\"Hamburg\"],\n",
    "    \"year\": [2023,2024,2023,2024,2023,2024],\n",
    "    \"sales\": [120, 150, 90, 110, 80, 130],\n",
    "    \"profit\": [12, 18, 8, 11, 7, 14]\n",
    "})\n",
    "\n",
    "# Boolean masks\n",
    "berlin_2024 = df.loc[(df[\"city\"]==\"Berlin\") & (df[\"year\"]==2024)]\n",
    "print(\"Berlin 2024:\\n\", berlin_2024, \"\\n\")\n",
    "\n",
    "# Between / isin\n",
    "big_years = df.loc[df[\"year\"].between(2023, 2024)]\n",
    "cities = df.loc[df[\"city\"].isin([\"Berlin\",\"Hamburg\"])]\n",
    "print(\"Years 2023-2024:\\n\", big_years, \"\\n\")\n",
    "print(\"Berlin & Hamburg:\\n\", cities, \"\\n\")\n",
    "\n",
    "# Fast scalar set with .at\n",
    "df2 = df.copy()\n",
    "df2.at[1, \"profit\"] = 19\n",
    "print(\"Scalar set with .at -> profit row 1 now:\", df2.loc[1, \"profit\"], \"\\n\")\n",
    "\n",
    "# MultiIndex\n",
    "mi = df.set_index([\"city\",\"year\"]).sort_index()\n",
    "print(\"MultiIndex head:\\n\", mi.head(), \"\\n\")\n",
    "\n",
    "# Slice MultiIndex: all rows for Berlin\n",
    "print(\"All Berlin via MultiIndex slice:\\n\", mi.loc[(\"Berlin\", slice(None))], \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f634f66",
   "metadata": {},
   "source": [
    "# 2) GroupBy Power Moves\n",
    "\n",
    "Patterns you'll use a lot:\n",
    "\n",
    "- Named aggregations: `groupby(...).agg(total=('col','sum'), avg=('col','mean'))`\n",
    "- Use `.transform` to broadcast group stats back to rows (e.g., z-scores per group).\n",
    "- `.filter` keeps/discards entire groups based on a condition.\n",
    "- `value_counts()` inside groups via `groupby('g')['x'].value_counts()`.\n",
    "- `rank` within groups for top-k selections."
   ]
  },
  {
   "cell_type": "code",
   "id": "9022c050",
   "metadata": {},
   "source": [
    "\n",
    "gdf = df.copy()\n",
    "\n",
    "# Named aggregations\n",
    "agg = gdf.groupby(\"city\").agg(\n",
    "    total_sales=(\"sales\",\"sum\"),\n",
    "    avg_profit=(\"profit\",\"mean\"),\n",
    "    count=(\"sales\",\"size\")\n",
    ")\n",
    "print(\"Named agg per city:\\n\", agg, \"\\n\")\n",
    "\n",
    "# Transform: z-score of sales per city\n",
    "gdf[\"sales_z_in_city\"] = gdf.groupby(\"city\")[\"sales\"].transform(\n",
    "    lambda s: (s - s.mean())/s.std(ddof=0)\n",
    ")\n",
    "print(\"Transform z-score per city:\\n\", gdf, \"\\n\")\n",
    "\n",
    "# Filter: keep only cities with total sales > 200\n",
    "filtered = gdf.groupby(\"city\").filter(lambda d: d[\"sales\"].sum() > 200)\n",
    "print(\"Filter groups (total sales > 200):\\n\", filtered, \"\\n\")\n",
    "\n",
    "# Value counts within groups\n",
    "vc = gdf.groupby(\"city\")[\"year\"].value_counts()\n",
    "print(\"Counts of year within city:\\n\", vc, \"\\n\")\n",
    "\n",
    "# Rank within groups (highest sales = rank 1)\n",
    "gdf[\"rank_in_city\"] = gdf.groupby(\"city\")[\"sales\"].rank(ascending=False, method=\"dense\")\n",
    "print(\"Rank within city:\\n\", gdf.sort_values([\"city\",\"rank_in_city\"]), \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6b99970",
   "metadata": {},
   "source": [
    "# 3) Reshaping: pivot, pivot_table, melt, crosstab\n",
    "\n",
    "When moving between long ↔ wide formats:\n",
    "\n",
    "- `pivot(index, columns, values)` purely reshapes (no aggregation).\n",
    "- `pivot_table(..., aggfunc='sum', fill_value=0)` reshapes **with** aggregation.\n",
    "- `melt` unpivots wide → long.\n",
    "- `pd.crosstab(rows, cols, values=..., aggfunc=...)` for contingency/summary tables."
   ]
  },
  {
   "cell_type": "code",
   "id": "8676f052",
   "metadata": {},
   "source": [
    "\n",
    "wide = df.pivot(index=\"city\", columns=\"year\", values=\"sales\")\n",
    "print(\"pivot (wide):\\n\", wide, \"\\n\")\n",
    "\n",
    "pv = pd.pivot_table(df, index=\"city\", columns=\"year\", values=\"profit\", aggfunc=\"sum\", fill_value=0)\n",
    "print(\"pivot_table sum of profit:\\n\", pv, \"\\n\")\n",
    "\n",
    "long = wide.reset_index().melt(id_vars=\"city\", var_name=\"year\", value_name=\"sales\")\n",
    "print(\"melt back to long:\\n\", long, \"\\n\")\n",
    "\n",
    "xt = pd.crosstab(df[\"city\"], df[\"year\"], values=df[\"sales\"], aggfunc=\"sum\")\n",
    "print(\"crosstab sum of sales:\\n\", xt, \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b773192e",
   "metadata": {},
   "source": [
    "# 4) Merge / Join Mastery\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Always know your join keys; use `validate='one_to_one'` or `validate='one_to_many'` to catch mistakes.\n",
    "- `indicator=True` shows where rows came from (`left_only`, `both`, `right_only`).\n",
    "- Use `suffixes=('_l','_r')` to disambiguate overlapping columns.\n",
    "- `merge_asof` is great for time‑aware nearest matches (sorted keys!)."
   ]
  },
  {
   "cell_type": "code",
   "id": "fe228b3d",
   "metadata": {},
   "source": [
    "\n",
    "left = pd.DataFrame({\"id\":[1,2,3], \"city\":[\"Berlin\",\"Munich\",\"Hamburg\"]})\n",
    "right = pd.DataFrame({\"id\":[1,2,4], \"year\":[2024,2023,2024]})\n",
    "\n",
    "inner = left.merge(right, on=\"id\", how=\"inner\", indicator=True, validate=\"one_to_one\")\n",
    "print(\"Inner merge with indicator:\\n\", inner, \"\\n\")\n",
    "\n",
    "outer = left.merge(right, on=\"id\", how=\"outer\", indicator=True, suffixes=(\"_l\",\"_r\"))\n",
    "print(\"Outer merge with suffixes:\\n\", outer, \"\\n\")\n",
    "\n",
    "# asof example: align timestamps to last seen event\n",
    "ts1 = pd.DataFrame({\"t\": pd.to_datetime([\"2025-01-01\",\"2025-01-01 12:00\",\"2025-01-02\"]), \"price\":[10, 11, 12]}).sort_values(\"t\")\n",
    "ts2 = pd.DataFrame({\"t\": pd.to_datetime([\"2025-01-01 06:00\",\"2025-01-01 18:00\",\"2025-01-02 06:00\"])}).sort_values(\"t\")\n",
    "asof = pd.merge_asof(ts2, ts1, on=\"t\", direction=\"backward\")\n",
    "print(\"merge_asof backward:\\n\", asof, \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6a555ed",
   "metadata": {},
   "source": [
    "# 5) Time Series Essentials\n",
    "\n",
    "Core operations:\n",
    "\n",
    "- Parse dates with `pd.to_datetime` and set as index for time‑aware ops.\n",
    "- `resample('D'/'W'/'M')` to change frequency.\n",
    "- `rolling(window).mean()` for moving averages; `expanding().sum()` for cumulative.\n",
    "- `shift` for lags/leads; `diff` for first differences."
   ]
  },
  {
   "cell_type": "code",
   "id": "0746d333",
   "metadata": {},
   "source": [
    "\n",
    "idx = pd.date_range(\"2025-01-01\", periods=7, freq=\"D\")\n",
    "ts = pd.DataFrame({\"date\": idx, \"value\": [5,6,7,12,8,9,15]}).set_index(\"date\")\n",
    "\n",
    "print(\"Original:\\n\", ts, \"\\n\")\n",
    "print(\"Weekly sum via resample:\\n\", ts[\"value\"].resample(\"W\").sum(), \"\\n\")\n",
    "print(\"3-day rolling mean:\\n\", ts[\"value\"].rolling(3).mean(), \"\\n\")\n",
    "print(\"Lag 1 with shift:\\n\", ts[\"value\"].shift(1), \"\\n\")\n",
    "print(\"First difference:\\n\", ts[\"value\"].diff(), \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84ccd1f4",
   "metadata": {},
   "source": [
    "# 6) Missing Data & Nullable dtypes\n",
    "\n",
    "Modern Pandas has nullable types like `'Int64'` (note the capital I).\n",
    "\n",
    "- Detect with `.isna()`/`.notna()`; count via `.isna().sum()`.\n",
    "- Impute with `fillna(value)` or `fillna(method='ffill'|'bfill')`.\n",
    "- Interpolate numeric series with `.interpolate()`.\n",
    "- Use `dropna(subset=[...])` to drop rows if certain columns are NA."
   ]
  },
  {
   "cell_type": "code",
   "id": "4669cfec",
   "metadata": {},
   "source": [
    "\n",
    "s = pd.Series([1, None, 3], dtype=\"Int64\")\n",
    "print(\"Nullable Int64:\\n\", s, s.dtype, \"\\n\")\n",
    "\n",
    "df_na = pd.DataFrame({\"a\":[1,2,None,4], \"b\":[10,None,30,40]})\n",
    "print(\"NA counts:\\n\", df_na.isna().sum(), \"\\n\")\n",
    "\n",
    "print(\"Fillna with scalar:\\n\", df_na.fillna(0), \"\\n\")\n",
    "print(\"Forward fill:\\n\", df_na.fillna(method=\"ffill\"), \"\\n\")\n",
    "print(\"Interpolate column b:\\n\", df_na[\"b\"].interpolate(), \"\\n\")\n",
    "\n",
    "print(\"Drop rows where a is NA:\\n\", df_na.dropna(subset=[\"a\"]), \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c6433cd",
   "metadata": {},
   "source": [
    "# 7) String Ops & Categories\n",
    "\n",
    "Vectorized string tools live under `.str` and are super fast.\n",
    "\n",
    "- `.str.contains`, `.str.extract` (regex), `.str.split`, and `.explode()` for list‑like cells.\n",
    "- Convert to categories to save memory and speed up groupby/joins when cardinality is low."
   ]
  },
  {
   "cell_type": "code",
   "id": "809fe101",
   "metadata": {},
   "source": [
    "\n",
    "names = pd.Series([\"Ada Lovelace\",\"Grace Hopper\",\"Alan Turing\",\"Linus Torvalds\"])\n",
    "print(\"Contains 'ing':\\n\", names.str.contains(\"ing\"), \"\\n\")\n",
    "print(\"Extract last name:\\n\", names.str.extract(r\"\\s([A-Za-z\\-]+)$\"), \"\\n\")\n",
    "\n",
    "tags = pd.Series([\"a,b\", \"a\", \"b,c\", \"a,c\"])\n",
    "split_exploded = tags.str.split(\",\").explode()\n",
    "print(\"Exploded tags:\\n\", split_exploded.value_counts(), \"\\n\")\n",
    "\n",
    "cat = pd.Series([\"small\",\"medium\",\"small\",\"large\"], dtype=\"category\")\n",
    "print(\"Category dtype:\", cat.dtype)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4629c36d",
   "metadata": {},
   "source": [
    "# 8) Performance & Memory Tips\n",
    "\n",
    "- Inspect memory with `df.info(memory_usage='deep')`.\n",
    "- Downcast numerics: `pd.to_numeric(..., downcast='unsigned'|'integer'|'float')`.\n",
    "- Prefer vectorized ops over `apply` in hot loops.\n",
    "- Use `.query()` / `.eval()` for readable filters on large frames.\n",
    "- Call `.to_numpy()` when you really need NumPy arrays for speed."
   ]
  },
  {
   "cell_type": "code",
   "id": "e8aceee4",
   "metadata": {},
   "source": [
    "\n",
    "big = pd.DataFrame({\n",
    "    \"a\": pd.to_numeric(np.random.randint(0, 1000, size=10_000), downcast=\"unsigned\"),\n",
    "    \"b\": pd.to_numeric(np.random.randn(10_000), downcast=\"float\")\n",
    "})\n",
    "# Show info (truncated in some environments)\n",
    "big.info(memory_usage=\"deep\")\n",
    "\n",
    "# query / eval examples\n",
    "res = big.query(\"a < 10 and b > 0\")\n",
    "res2 = big.eval(\"c = a * b\")\n",
    "print(\"query rows:\", len(res), \"| eval new col head:\\n\", res2.head(), \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "07290ad5",
   "metadata": {},
   "source": [
    "# 9) Method Chaining & pipe\n",
    "\n",
    "Write tidy, readable pipelines with `.pipe` and line breaks:\n",
    "\n",
    "```python\n",
    "(out := (\n",
    "    df\n",
    "      .query(\"year >= 2023\")\n",
    "      .assign(margin=lambda d: d.profit / d.sales)\n",
    "      .pipe(lambda d: d.sort_values(['city','margin'], ascending=[True, False]))\n",
    "      .reset_index(drop=True)\n",
    "))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "06593138",
   "metadata": {},
   "source": [
    "\n",
    "out = (\n",
    "    df\n",
    "    .query(\"year >= 2023\")\n",
    "    .assign(margin=lambda d: d[\"profit\"] / d[\"sales\"])\n",
    "    .pipe(lambda d: d.sort_values([\"city\",\"margin\"], ascending=[True, False]))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(out, \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c057d233",
   "metadata": {},
   "source": [
    "# 10) Data Quality Checks\n",
    "\n",
    "Common checks to add early in your workflow:\n",
    "\n",
    "- Duplicates: `df.duplicated(subset=[...])` → then `drop_duplicates`.\n",
    "- Ranges: `df['age'].between(0,120)`.\n",
    "- Schema: `df.dtypes` vs expected, `set(expected_cols) - set(df)`.\n",
    "- Equality: `df1.equals(df2)` strictly compares values & dtypes."
   ]
  },
  {
   "cell_type": "code",
   "id": "2151b0c6",
   "metadata": {},
   "source": [
    "\n",
    "dq = pd.DataFrame({\n",
    "    \"id\": [1,2,2,3],\n",
    "    \"age\": [25, -5, 40, 130]\n",
    "})\n",
    "print(\"Duplicate mask:\\n\", dq.duplicated(subset=[\"id\"]), \"\\n\")\n",
    "print(\"Valid age mask (0..120):\\n\", dq[\"age\"].between(0,120), \"\\n\")\n",
    "dq_clean = dq.drop_duplicates(subset=[\"id\"]).query(\"age.between(0,120)\")\n",
    "print(\"Cleaned:\\n\", dq_clean, \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5141ee9",
   "metadata": {},
   "source": [
    "# 12) MultiIndex Patterns: stack/unstack, swaplevel\n",
    "\n",
    "- `unstack` pivots the **inner** index level to columns.\n",
    "- `stack` goes the other way.\n",
    "- `swaplevel` reorders levels; `sort_index` stabilizes after swaps."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5ca29c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T10:22:34.947065Z",
     "start_time": "2025-10-12T10:22:34.858176Z"
    }
   },
   "source": [
    "\n",
    "mi2 = df.set_index([\"city\",\"year\"]).sort_index()\n",
    "unstacked = mi2[\"sales\"].unstack(\"year\")\n",
    "print(\"Unstacked sales:\\n\", unstacked, \"\\n\")\n",
    "\n",
    "restacked = unstacked.stack()\n",
    "print(\"Restacked equals original values:\", np.allclose(restacked.values, mi2[\"sales\"].values), \"\\n\")\n",
    "\n",
    "swapped = mi2.swaplevel(0,1).sort_index()\n",
    "print(\"Swap levels:\\n\", swapped.head(), \"\\n\")\n"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['city', 'year'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_7960\\1043806358.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m mi2 = df.set_index([\u001B[33m\"city\"\u001B[39m,\u001B[33m\"year\"\u001B[39m]).sort_index()\n\u001B[32m      2\u001B[39m unstacked = mi2[\u001B[33m\"sales\"\u001B[39m].unstack(\u001B[33m\"year\"\u001B[39m)\n\u001B[32m      3\u001B[39m print(\u001B[33m\"Unstacked sales:\\n\"\u001B[39m, unstacked, \u001B[33m\"\\n\"\u001B[39m)\n\u001B[32m      4\u001B[39m \n",
      "\u001B[32mC:\\GitHub\\data-science\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, keys, drop, append, inplace, verify_integrity)\u001B[39m\n\u001B[32m   6140\u001B[39m                     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m found:\n\u001B[32m   6141\u001B[39m                         missing.append(col)\n\u001B[32m   6142\u001B[39m \n\u001B[32m   6143\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m missing:\n\u001B[32m-> \u001B[39m\u001B[32m6144\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m KeyError(f\"None of {missing} are in the columns\")\n\u001B[32m   6145\u001B[39m \n\u001B[32m   6146\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[32m   6147\u001B[39m             frame = self\n",
      "\u001B[31mKeyError\u001B[39m: \"None of ['city', 'year'] are in the columns\""
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
