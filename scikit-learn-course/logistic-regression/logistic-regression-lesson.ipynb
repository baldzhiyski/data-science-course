{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem Statement\n",
    "\n",
    "\n",
    "\n",
    "> **QUESTION**: The [Rain in Australia dataset](https://kaggle.com/jsphyg/weather-dataset-rattle-package) contains about 10 years of daily weather observations from numerous Australian weather stations.\n",
    "> As a data scientist at the Bureau of Meteorology, you are tasked with creating a fully-automated system that can use today's weather data for a given location to predict whether it will rain at the location tomorrow.\n",
    "\n",
    "\n",
    "**EXERCISE**: Before proceeding further, take a moment to think about how you can approach this problem. List five or more ideas that come to your mind below:\n",
    "\n",
    "1. Do basic EDA: check missing values, class balance (how many “RainTomorrow = Yes/No”), look for obvious predictors like RainToday, Humidity, Cloud, Pressure.\n",
    "2. Build a simple baseline model: predict “No” for everyone (or always use today’s rain → tomorrow’s rain), to have something to beat.\n",
    "3. Create features from today’s weather: e.g. Humidity3pm, RainToday, Temp3pm, WindGustSpeed, Pressure3pm — and train a classification model (Logistic Regression / RandomForest) to predict RainTomorrow.\n",
    "4. Handle missing data properly: impute numeric cols (median/mean), fill/categorize missing categorical values, maybe drop columns with too many NaNs.\n",
    "5. Encode categorical columns (Location, WindDir9am, WindDir3pm, WindGustDir) using one-hot encoding.\n",
    "6. Split into train/test by **date** (older → train, newer → test) to simulate real forecasting.\n",
    "7. Evaluate with accuracy **and** recall/precision for the “RainTomorrow = Yes” class (it’s usually imbalanced)."
   ],
   "id": "cd469a7dbfa3d9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linear Regression vs. Logistic Regression\n",
    "\n",
    "When predicting **continuous values** (like medical charges), we use **Linear Regression**.\n",
    "When predicting **categories or classes** (like rain vs no rain), we use **Logistic Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "| Aspect | Linear Regression | Logistic Regression |\n",
    "|:--------|:------------------|:--------------------|\n",
    "| **Goal** | Predict a continuous **numeric value** | Predict a **category/class** (e.g., Rain/No Rain) |\n",
    "| **Output** | Any real number (−∞ to +∞) | A probability between 0 and 1 |\n",
    "| **Typical Use** | Price, temperature, salary, medical costs | Spam detection, disease diagnosis, rainfall prediction |\n",
    "| **Decision Boundary** | Continuous value, no threshold | Converts probability to class label using threshold (e.g., 0.5) |\n",
    "| **Loss Function** | Mean Squared Error (MSE) | Binary Cross-Entropy (Log Loss) |\n",
    "| **Assumption** | Linear relationship between X and y | Classes are separable in feature space |\n",
    "| **Interpretation** | Predicts *how much* | Predicts *which class* |\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Form\n",
    "\n",
    "**Linear Regression:**\n",
    "\n",
    "$$\n",
    "\\hat{y} = w^T x + b\n",
    "$$\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "where the **sigmoid (logistic)** function is:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This ensures that the output probability \\( \\hat{p} \\) always lies between **0 and 1**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Linear Regression** → Best for predicting continuous outcomes\n",
    "- **Logistic Regression** → Best for predicting binary (yes/no) or categorical outcomes\n",
    "\n"
   ],
   "id": "5f40214f83fb1c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading the Data\n",
   "id": "101ff9f6be2c9744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "df.head()"
   ],
   "id": "cee8925b0892863f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset contains over 145,000 rows and 23 columns. The dataset contains date, numeric and categorical columns. Our objective is to create a model to predict the value in the column `RainTomorrow`.\n",
    "\n",
    "Let's check the data types and missing values in the various columns."
   ],
   "id": "9e0a7e7b97dd2b8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "4f1dcce256462d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)",
   "id": "ae526d4ae45a09cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis and Visualization\n",
    "\n",
    "Before training a machine learning model, its always a good idea to explore the distributions of various columns and see how they are related to the target column. Let's explore and visualize the data using the Plotly, Matplotlib and Seaborn libraries."
   ],
   "id": "cd5a16e82c415759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ],
   "id": "b347bf18c05bb1ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.histogram(df, x='Location', title='Location vs. Rainy Days', color='RainToday')",
   "id": "2113b6e1597e3069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.histogram(df,\n",
    "             x='Temp3pm',\n",
    "             title='Temp at 3pm vs. Rainy Days',\n",
    "             color='RainTomorrow')"
   ],
   "id": "a61e61bdf696d54f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.histogram(df,\n",
    "             x='RainTomorrow',\n",
    "             color='RainToday',\n",
    "             title='Rain Tomorrow vs. Rain Today')"
   ],
   "id": "a077ac48a20b5ac1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           title='Min Temp. vs Max Temp.',\n",
    "           x='MinTemp',\n",
    "           y='MaxTemp',\n",
    "           color='RainToday')"
   ],
   "id": "a46f525db2aa11ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "px.scatter(df.sample(2000),\n",
    "           title='Temp at 3 pm vs. Humidity (3 pm)',\n",
    "           x='Temp3pm',\n",
    "           y='Humidity3pm',\n",
    "           color='RainTomorrow')"
   ],
   "id": "fcbddf3861b54519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> **EXERCISE**: Visualize all the other columns of the dataset and study their relationship with the `RainToday` and `RainTomorrow` columns.",
   "id": "dbb2085d69c9f4eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- NUMERIC FEATURES ---\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "num_features = [\n",
    "    'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
    "    'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n",
    "    'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm',\n",
    "    'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'\n",
    "]\n",
    "\n",
    "# Create small box plots for numeric columns vs RainTomorrow\n",
    "for col in num_features:\n",
    "    fig = px.box(\n",
    "        df, x='RainTomorrow', y=col, color='RainTomorrow',\n",
    "        title=f\"{col} vs RainTomorrow\", height=300, width=400\n",
    "    )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.show()\n",
    "\n",
    "# --- CATEGORICAL FEATURES ---\n",
    "cat_features = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
    "\n",
    "for col in cat_features:\n",
    "    fig = px.histogram(\n",
    "        df, x=col, color='RainTomorrow', barmode='group',\n",
    "        title=f\"{col} distribution by RainTomorrow\", height=300, width=500\n",
    "    )\n",
    "    fig.update_xaxes(categoryorder='total descending')\n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.show()\n",
    "\n",
    "# --- OPTIONAL COMPACT GRID FOR KEY FEATURES ---\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=[\n",
    "    \"Humidity3pm vs RainTomorrow\",\n",
    "    \"Pressure3pm vs RainTomorrow\",\n",
    "    \"WindSpeed3pm vs RainTomorrow\",\n",
    "    \"Temp3pm vs RainTomorrow\"\n",
    "])\n",
    "\n",
    "features = ['Humidity3pm', 'Pressure3pm', 'WindSpeed3pm', 'Temp3pm']\n",
    "r, c = 1, 1\n",
    "\n",
    "for f in features:\n",
    "    box = px.box(df, x='RainTomorrow', y=f, color='RainTomorrow')\n",
    "    for trace in box.data:\n",
    "        fig.add_trace(trace, row=r, col=c)\n",
    "    c += 1\n",
    "    if c == 3:\n",
    "        c = 1\n",
    "        r += 1\n",
    "\n",
    "fig.update_layout(height=700, width=850, title_text=\"Key Weather Features vs RainTomorrow\", showlegend=False)\n",
    "fig.show()\n"
   ],
   "id": "88c332cc56ccbb19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "use_sample = False\n",
    "\n",
    "sample_fraction = 0.1\n",
    "\n",
    "if use_sample:\n",
    "    df = df.sample(frac=sample_fraction).copy()"
   ],
   "id": "51c01507e9274d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training, Validation and Test Sets\n",
    "\n",
    "While building real-world machine learning models, it is quite common to split the dataset into three parts:\n",
    "\n",
    "1. **Training set** - used to train the model, i.e., compute the loss and adjust the model's weights using an optimization technique.\n",
    "\n",
    "\n",
    "2. **Validation set** - used to evaluate the model during training, tune model hyperparameters (optimization technique, regularization etc.), and pick the best version of the model. Picking a good validation set is essential for training models that generalize well.\n",
    "\n",
    "3. **Test set** - used to compare different models or approaches and report the model's final accuracy. For many datasets, test sets are provided separately. The test set should reflect the kind of data the model will encounter in the real-world, as closely as feasible.\n",
    "\n",
    "\n",
    "As a general rule of thumb you can use around 60% of the data for the training set, 20% for the validation set and 20% for the test set. If a separate test set is already provided, you can use a 75%-25% training-validation split.\n"
   ],
   "id": "48176ced61ba7f04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split",
   "id": "818e2b50a2227d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)"
   ],
   "id": "794a65f833a753e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('train_df.shape :', train_df.shape)\n",
    "print('val_df.shape :', val_df.shape)\n",
    "print('test_df.shape :', test_df.shape)"
   ],
   "id": "a50c401575cfc5b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.title('No. of Rows per Year')\n",
    "sns.countplot(x=pd.to_datetime(df.Date).dt.year);"
   ],
   "id": "fa809f58c99f1b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "year = pd.to_datetime(df.Date).dt.year\n",
    "\n",
    "train_df = df[year < 2015]\n",
    "val_df = df[year == 2015]\n",
    "test_df = df[year > 2015]"
   ],
   "id": "64990998c1fa463e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('train_df.shape :', train_df.shape)\n",
    "print('val_df.shape :', val_df.shape)\n",
    "print('test_df.shape :', test_df.shape)"
   ],
   "id": "e13a2ced5ccc3465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Identifying Input and Target Columns\n",
    "\n",
    "Often, not all the columns in a dataset are useful for training a model. In the current dataset, we can ignore the `Date` column, since we only want to weather conditions to make a prediction about whether it will rain the next day.\n",
    "\n",
    "Let's create a list of input columns, and also identify the target column."
   ],
   "id": "890612a426dcf97f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_cols = list(train_df.columns)[1:-1]\n",
    "target_col = 'RainTomorrow'\n",
    "print(input_cols)"
   ],
   "id": "d217cfdf3beb2191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now create inputs and targets for the training, validation and test sets for further processing and model training.",
   "id": "e4ecf0bb4794726d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_inputs = train_df[input_cols].copy()\n",
    "train_targets = train_df[target_col].copy()\n",
    "\n",
    "val_inputs = val_df[input_cols].copy()\n",
    "val_targets = val_df[target_col].copy()\n",
    "\n",
    "test_inputs = test_df[input_cols].copy()\n",
    "test_targets = test_df[target_col].copy()"
   ],
   "id": "a1b74073764fcb9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's also identify which of the columns are numerical and which ones are categorical. This will be useful later, as we'll need to convert the categorical data to numbers for training a logistic regression model.",
   "id": "ec4cead5535e42c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = train_inputs.select_dtypes('object').columns.tolist()"
   ],
   "id": "77db829e791821f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_inputs[numeric_cols].describe()",
   "id": "fcaa08f145c8ab2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_inputs[categorical_cols].nunique()",
   "id": "d40675f635771aa4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
